# Домашнее задание 1. web crawler
## Описание
Web crawler обходит сайт в гулибну и скачивает его страницы.
Ссылки хранятся в файле links.txt, а html страницы с соответсвующими id в названии хранятся в папке data

## Описание работы
Краулер запускается из терминала с помощью команды:
python hw_1.py arg1 arg2
 где arg1 - это ссылка на сайт, который мы собираемся обходить
     arg2 - глубина обхода этого сайта

Во время парсинга страницы:
    -если ссылка не относится к нашему сайту, то ее пропускает
    -если ссылка относится к нашему сайту и ее мы не парсили ранее, то мы добавляем ее в файл links.txt и ее html страницу добавляем в папку data и скрипт парсит уже эту страницу. И так до тех пор, пока скрипт не дойдет до указанной глубины
    -если на запрос мы получаем код 4хх или 5хх или ConnectionError, то такую ссылку скрипт тоже пропускает

Если во время запуска скрипта вы в неверном формате указали ссылку или глубину обхода, то вам предложат ввести корректные данные

Скрипт тестировался на сайте 'https://ycombinator.com/'
